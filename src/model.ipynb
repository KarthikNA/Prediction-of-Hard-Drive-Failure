{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from a combined file with both good and failed drives\n",
    "def split_train_val_test_data( root = \"../dataset\", drive_file = \"/ST12000NM0007_last_10_day.csv\",  \n",
    "                          ignore_cols = [\"date\",\"serial_number\",\"model\",\"capacity_bytes\",\"failure\"], \n",
    "                          resample_data=False, smote_data=False):\n",
    "\n",
    "    df = pd.read_csv(root+drive_file, parse_dates=True)\n",
    "\n",
    "    df_good = df.loc[df['failure'] == 0]\n",
    "    df_bad = df.loc[df['failure'] == 1]\n",
    "     \n",
    "    df_good = df_good.sort_values([\"date\"])\n",
    "    df_bad = df_bad.sort_values([\"date\"])\n",
    "\n",
    "    good_y = df_good[\"failure\"]\n",
    "    bad_y = df_bad[\"failure\"]\n",
    "\n",
    "    # Split into train (80%) and test (20%)\n",
    "    X_train_good, X_test_good, y_train_good, y_test_good = train_test_split(\n",
    "        df_good, good_y, train_size=0.8, shuffle=False)\n",
    "    X_train_bad, X_test_bad, y_train_bad, y_test_bad = train_test_split(\n",
    "        df_bad, bad_y, train_size=0.8, shuffle=False)\n",
    "\n",
    "\n",
    "    # Split train into train and validation\n",
    "    # Train(60%), Val(20%), Test(20%)\n",
    "    X_train_good, X_val_good, y_train_good, y_val_good = train_test_split(\n",
    "        X_train_good, y_train_good, train_size=0.75, shuffle=False)\n",
    "    X_train_bad, X_val_bad, y_train_bad, y_val_bad = train_test_split(\n",
    "        X_train_bad, y_train_bad, train_size=0.75, shuffle=False)\n",
    "        \n",
    "    if resample_data:\n",
    "        X_train_bad = resample(df_bad, replace=True, n_samples=len(X_train_good), random_state=1)\n",
    "        X_train_bad = X_train_bad.sort_values([\"date\"])\n",
    "\n",
    "    y_train_bad = X_train_bad[\"failure\"]\n",
    "\n",
    "    X_train = pd.concat([X_train_good, X_train_bad], axis=0)\n",
    "    y_train = pd.concat([y_train_good, y_train_bad], axis=0)\n",
    "    X_val = pd.concat([X_val_good, X_val_bad], axis=0)\n",
    "    y_val = pd.concat([y_val_good, y_val_bad], axis=0)\n",
    "    X_test = pd.concat([X_test_good, X_test_bad], axis=0)\n",
    "    y_test = pd.concat([y_test_good, y_test_bad], axis=0)\n",
    "\n",
    "    X_train.drop(columns=ignore_cols, inplace=True, axis=1)\n",
    "    X_val.drop(columns=ignore_cols, inplace=True, axis=1)\n",
    "    X_test.drop(columns=ignore_cols, inplace=True, axis=1)\n",
    "\n",
    "    if smote_data:\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    # return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting data by date\n",
    "def sort_data_by_date(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=True)\n",
    "    sorted_df = df.sort_values([\"date\"])\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_n_estimators(model, X_train, y_train):\n",
    "    print(\"Getting optimal n_estimators! \")\n",
    "    import xgboost as xgb\n",
    "    # 1: Set learning rate and n_estimators\n",
    "    xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "    params = model.get_xgb_params()\n",
    "    num_boost_round = model.get_params()['n_estimators'] \n",
    "    metrics = 'auc'\n",
    "    nfold = 5\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    \n",
    "    cvresult = xgb.cv(params, xgtrain, num_boost_round = num_boost_round, nfold = nfold,\n",
    "                      metrics = metrics, early_stopping_rounds = early_stopping_rounds)\n",
    "    \n",
    "    n_estimators = cvresult.shape[0]\n",
    "    print(\"n_estimators: \", n_estimators)\n",
    "    return n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_max_dep_min_child_weight(n_estimators, X_train, y_train):\n",
    "    print(\"Getting optimal max_depth and min_child_weight!\")\n",
    "    param = {\n",
    "        'max_depth': range(3,10,2),\n",
    "        'min_child_weight': range(1,6,2)\n",
    "    }\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = XGBClassifier( learning_rate = 0.1, n_estimators = n_estimators, \n",
    "                                                      max_depth = 5, min_child_weight = 1, gamma = 0, \n",
    "                                                      subsample = 0.8, colsample_bytree = 0.8,\n",
    "                                                      objective = 'binary:logistic', scale_pos_weight = 1, seed = 27), \n",
    "                                                      param_grid = param, scoring = ['f1','accuracy'], refit='f1', iid = False, cv = 5)\n",
    "    gsearch.fit(X_train, y_train)\n",
    "    max_depth = gsearch.best_params_['max_depth']\n",
    "    min_child_weight = gsearch.best_params_['min_child_weight']\n",
    "    print(\"Best params: \", gsearch.best_params_)\n",
    "    return (max_depth, min_child_weight)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_gamma(n_estimators, max_depth, min_child_weight, X_train, y_train):\n",
    "    print(\"Getting optimal gamma!\")\n",
    "    \n",
    "    param = {\n",
    "        'gamma':[i/10.0 for i in range(0,5)]\n",
    "    }\n",
    "    gsearch = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, n_estimators = n_estimators, \n",
    "                                                      max_depth = max_depth, min_child_weight = min_child_weight,\n",
    "                                                      gamma = 0, subsample = 0.8, colsample_bytree = 0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight = 1, seed = 27), \n",
    "                                                      param_grid = param,  scoring = ['f1','accuracy'], refit='f1',iid = False, cv = 5)\n",
    "    gsearch.fit(X_train, y_train)\n",
    "    gamma = gsearch.best_params_['gamma']\n",
    "    print(\"Best params: \", gsearch.best_params_)\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_reg_params(n_estimators, max_depth, min_child_weight, gamma, X_train, y_train):\n",
    "    print(\"Getting optimal reg_alpha!\")\n",
    "    param = {\n",
    "        'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators = n_estimators, \n",
    "                                                      max_depth = max_depth, min_child_weight = min_child_weight,\n",
    "                                                      gamma = gamma, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                     objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    "                                                       param_grid = param, scoring = ['f1','accuracy'], refit='f1', iid=False, cv=5)\n",
    "    gsearch.fit(X_train, y_train)\n",
    "    reg_alpha = gsearch.best_params_['reg_alpha']\n",
    "    print(\"Best params: \", gsearch.best_params_)\n",
    "    return reg_alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_xgb(model, X_train, y_train):\n",
    "    print(\"Start tuning\")\n",
    "    # 1: Tuning n_estimators\n",
    "    n_estimators = tune_n_estimators(model, X_train, y_train)\n",
    "    \n",
    "    # 2:  Tuning max_depth and min_child_weight\n",
    "    max_depth, min_child_weight = tune_max_dep_min_child_weight(n_estimators, X_train, y_train)\n",
    "    \n",
    "    # 3: Tuning gamma\n",
    "    gamma = tune_gamma(n_estimators, max_depth, min_child_weight, X_train, y_train)\n",
    "    \n",
    "    # 4: Tuning regularization paramaters\n",
    "    reg_alpha = tune_reg_params(n_estimators, max_depth, min_child_weight, gamma, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(models = [RandomForestClassifier(max_depth=2, random_state=0)], tune_model=False):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test_data(drive_file = \"/TOSHIBA MQ01ABF050_last_10_day_all_q_raw.csv\", resample_data=True)\n",
    "#     X_train, X_test, y_train, y_test = split_train_val_test_data(drive_file = \"/ST8000DM002_last_10_day_all_q_raw.csv\", resample_data=True)\n",
    "    #X_train, X_test, y_train, y_test = get_train_test_data(resample_data=True)\n",
    "    print(\"Data loaded successfully...\\n\")\n",
    "    for model in models:  \n",
    "        print(\"\\n\\n *\", type(model).__name__)  \n",
    "        \n",
    "        if(type(model).__name__ == \"XGBClassifier\" and tune_model):\n",
    "            tune_xgb(model, X_train, y_train)\n",
    "\n",
    "        start = time()\n",
    "        model.fit(X_train, y_train)\n",
    "        end = time()\n",
    "        print(\"\\nTime to train:\", str((end - start)/60), \" mins\")\n",
    "        \n",
    "        # Validation set results\n",
    "        print(\"\\n- Results on validation set: \")\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        print(\"Accuracy: \", accuracy_score(y_val, y_val_pred))\n",
    "        print(\"Scores:\\n\", classification_report(y_val, y_val_pred))\n",
    "        \n",
    "        # Test set results\n",
    "        print(\"\\n- Results on test set: \")\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "        print(\"Scores:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully...\n",
      "\n",
      "\n",
      "\n",
      " * XGBClassifier\n",
      "\n",
      "Time to train: 0.019732332229614256  mins\n",
      "\n",
      "- Results on validation set: \n",
      "Accuracy:  1.0\n",
      "Scores:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       912\n",
      "           1       1.00      1.00      1.00       155\n",
      "\n",
      "    accuracy                           1.00      1067\n",
      "   macro avg       1.00      1.00      1.00      1067\n",
      "weighted avg       1.00      1.00      1.00      1067\n",
      "\n",
      "\n",
      "- Results on test set: \n",
      "Accuracy:  0.9990636704119851\n",
      "Scores:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       912\n",
      "           1       0.99      1.00      1.00       156\n",
      "\n",
      "    accuracy                           1.00      1068\n",
      "   macro avg       1.00      1.00      1.00      1068\n",
      "weighted avg       1.00      1.00      1.00      1068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entry point of function\n",
    "if __name__ == \"__main__\":\n",
    "    models_list = []\n",
    "    xgbc = XGBClassifier(learning_rate = 0.2,\n",
    "                         n_estimators=1251,\n",
    "                         max_depth=3,\n",
    "                         min_child_weight=3,\n",
    "                         gamma=0.1,\n",
    "                         reg_alpha=0.01,\n",
    "                         subsample=0.8,\n",
    "                         colsample_bytree=0.8,\n",
    "                         objective= 'binary:logistic',\n",
    "                         scale_pos_weight=1,\n",
    "                         seed=27)\n",
    "    \n",
    "    models_list.append(xgbc)\n",
    "    run(models_list,tune_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "#### ST12000NM0007_last_10_day_all_q_raw.csv\n",
    "\n",
    "\n",
    " * XGBClassifier\n",
    "\n",
    "Time to train: 3.739137363433838  mins\n",
    "\n",
    "- Results on validation set: \n",
    "Accuracy:  0.9919698673851063\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.99      1.00     74210\n",
    "           1       0.79      1.00      0.88      2252\n",
    "\n",
    "    accuracy                           0.99     76462\n",
    "   macro avg       0.89      1.00      0.94     76462\n",
    "weighted avg       0.99      0.99      0.99     76462\n",
    "\n",
    "\n",
    "- Results on test set: \n",
    "Accuracy:  0.9851821819989014\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.98      0.99     74210\n",
    "           1       0.67      1.00      0.80      2252\n",
    "\n",
    "    accuracy                           0.99     76462\n",
    "   macro avg       0.83      0.99      0.90     76462\n",
    "weighted avg       0.99      0.99      0.99     76462\n",
    "\n",
    "\n",
    "#### ST4000DM000_last_10_day_all_q_raw.csv\n",
    "\n",
    " * XGBClassifier\n",
    "\n",
    "Time to train: 2.175768995285034  mins\n",
    "\n",
    "- Results on validation set: \n",
    "Accuracy:  0.9982876712328768\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     45926\n",
    "           1       0.91      1.00      0.95       794\n",
    "\n",
    "    accuracy                           1.00     46720\n",
    "   macro avg       0.95      1.00      0.98     46720\n",
    "weighted avg       1.00      1.00      1.00     46720\n",
    "\n",
    "\n",
    "- Results on test set: \n",
    "Accuracy:  0.9941780821917808\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.99      1.00     45926\n",
    "           1       0.74      1.00      0.85       794\n",
    "\n",
    "    accuracy                           0.99     46720\n",
    "   macro avg       0.87      1.00      0.93     46720\n",
    "weighted avg       1.00      0.99      0.99     46720\n",
    "\n",
    "\n",
    "#### ST8000NM0055_last_10_day_all_q_raw.csv\n",
    "\n",
    "* XGBClassifier\n",
    "\n",
    "Time to train: 0.7112944483757019  mins\n",
    "\n",
    "- Results on validation set: \n",
    "Accuracy:  0.9999318383204963\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     28906\n",
    "           1       1.00      1.00      1.00       436\n",
    "\n",
    "    accuracy                           1.00     29342\n",
    "   macro avg       1.00      1.00      1.00     29342\n",
    "weighted avg       1.00      1.00      1.00     29342\n",
    "\n",
    "\n",
    "- Results on test set: \n",
    "Accuracy:  0.9997273532819849\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     28906\n",
    "           1       0.98      1.00      0.99       436\n",
    "\n",
    "    accuracy                           1.00     29342\n",
    "   macro avg       0.99      1.00      1.00     29342\n",
    "weighted avg       1.00      1.00      1.00     29342\n",
    "\n",
    "\n",
    "#### ST8000DM002_last_10_day_all_q_raw.csv\n",
    "\n",
    "* XGBClassifier\n",
    "\n",
    "Time to train: 0.47229440212249757  mins\n",
    "\n",
    "- Results on validation set: \n",
    "Accuracy:  0.9998993254807208\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     19627\n",
    "           1       0.99      1.00      1.00       239\n",
    "\n",
    "    accuracy                           1.00     19866\n",
    "   macro avg       1.00      1.00      1.00     19866\n",
    "weighted avg       1.00      1.00      1.00     19866\n",
    "\n",
    "\n",
    "- Results on test set: \n",
    "Accuracy:  0.9997483137018021\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     19627\n",
    "           1       0.98      1.00      0.99       239\n",
    "\n",
    "    accuracy                           1.00     19866\n",
    "   macro avg       0.99      1.00      0.99     19866\n",
    "weighted avg       1.00      1.00      1.00     19866\n",
    "\n",
    "#### TOSHIBA MQ01ABF050\n",
    "\n",
    "* XGBClassifier\n",
    "\n",
    "Time to train: 0.019732332229614256  mins\n",
    "\n",
    "- Results on validation set: \n",
    "Accuracy:  1.0\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       912\n",
    "           1       1.00      1.00      1.00       155\n",
    "\n",
    "    accuracy                           1.00      1067\n",
    "   macro avg       1.00      1.00      1.00      1067\n",
    "weighted avg       1.00      1.00      1.00      1067\n",
    "\n",
    "\n",
    "- Results on test set: \n",
    "Accuracy:  0.9990636704119851\n",
    "Scores:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       912\n",
    "           1       0.99      1.00      1.00       156\n",
    "\n",
    "    accuracy                           1.00      1068\n",
    "   macro avg       1.00      1.00      1.00      1068\n",
    "weighted avg       1.00      1.00      1.00      1068\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 66885, -1: 2877})\n",
      "Counter({0: 66885, 1: 2877})\n",
      "Scores:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     69419\n",
      "           1       0.02      0.19      0.04       343\n",
      "\n",
      "    accuracy                           0.96     69762\n",
      "   macro avg       0.51      0.57      0.51     69762\n",
      "weighted avg       0.99      0.96      0.97     69762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, _, y_test = split_train_val_test_data(resample_data=True)\n",
    "clf = IsolationForest(random_state=0, contamination=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_test) \n",
    "y_pred = [0 if x > 0 else 1 for x in y_pred]\n",
    "print(\"Scores:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.94     69419\n",
      "           1       0.02      0.41      0.04       343\n",
      "\n",
      "    accuracy                           0.90     69762\n",
      "   macro avg       0.51      0.65      0.49     69762\n",
      "weighted avg       0.99      0.90      0.94     69762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, _, y_test = split_train_val_test_data(resample_data=False)\n",
    "clf = IsolationForest(random_state=0, contamination=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_test) \n",
    "y_pred = [0 if x > 0 else 1 for x in y_pred]\n",
    "print(\"Scores:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
